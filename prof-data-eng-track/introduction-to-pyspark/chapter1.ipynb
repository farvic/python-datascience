{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01124b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(spark)\n",
    "\n",
    "# Read in the CSV\n",
    "census_adult = spark.read.csv(\"adult_reduced.csv\")\n",
    "\n",
    "# Show the DataFrame\n",
    "census_adult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f23d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "salaries_df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Count the total number of rows\n",
    "row_count = salaries_df.count()\n",
    "print(f\"Total rows: {row_count}\")\n",
    "\n",
    "# Group by company size and calculate the average of salaries\n",
    "salaries_df.groupBy(\"company_size\").agg({\"salary_in_usd\": \"avg\"}).show()\n",
    "salaries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average salary for entry level in Canada\n",
    "CA_jobs = ca_salaries_df.filter(ca_salaries_df['company_location'] == \"CA\").filter(ca_salaries_df['experience_level']\n",
    " == \"EN\").groupBy().avg(\"salary_in_usd\")\n",
    "\n",
    "# Show the result\n",
    "CA_jobs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Files: Common for structured, delimited data\n",
    "spark.read.csv(\"path/to/file.csv\")\n",
    "\n",
    "# JSON Files: Semi-structured, hierarchical data format\n",
    "\n",
    "spark.read.json(\"path/to/file.json\")\n",
    "\n",
    "# Parquet Files: Optimized for storage and querying, often used in data engineering\n",
    "\n",
    "spark.read.parquet(\"path/to/file.parquet\")\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/\n",
    "# /pyspark.pandas/api/pyspark.pandas.read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd730c5",
   "metadata": {},
   "source": [
    "## DataTypes Syntax for PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the necessary types as classes\n",
    "\n",
    "from pyspark.sql.types import (StructType, StructField, IntegerType, StringType, ArrayType)\n",
    "\n",
    "# Construct the schema\n",
    "schema = StructType([\n",
    "StructField(\"id\", IntegerType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"scores\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "# Set the schema\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d318c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and show only the name and age columns\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Filter on age > 30\n",
    "df.filter(df[\"age\"] > 30).show()\n",
    "\n",
    "# Use Where to filter match a specific value\n",
    "df.where(df[\"age\"] == 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c182bb",
   "metadata": {},
   "source": [
    "### Sorting and dropping missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Order data using .sort() or .orderBy()\n",
    "# Use na.drop() to remove rows with null values\n",
    "\n",
    "# Sort using the age column\n",
    "df.sort(\"age\", ascending=False).show()\n",
    "\n",
    "# Drop missing values\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "census_df = spark.read.json(\"adults.json\")\n",
    "\n",
    "# Filter rows based on age condition\n",
    "salary_filtered_census = census_df.filter(census_df[\"age\"] > 40)\n",
    "\n",
    "# Show the result\n",
    "salary_filtered_census.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Fill in the schema with the columns you need from the exercise instructions\n",
    "schema = StructType([StructField(\"age\",IntegerType()),\n",
    "                     StructField(\"education_num\",IntegerType()),\n",
    "                     StructField(\"marital_status\",StringType()),\n",
    "                     StructField(\"occupation\",StringType()),\n",
    "                     StructField(\"income\",StringType())\n",
    "                    ])\n",
    "\n",
    "# Read in the CSV, using the schema you defined above\n",
    "census_adult = spark.read.csv(\"adult_reduced_100.csv\", sep=',', header=False, schema=schema)\n",
    "\n",
    "# Print out the schema\n",
    "census_adult.printSchema()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
