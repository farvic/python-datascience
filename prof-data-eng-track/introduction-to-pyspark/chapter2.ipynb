{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9aa52e",
   "metadata": {},
   "source": [
    "## Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54166604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Use .na.drop() to remove rows with null values\n",
    "\n",
    "# Drop rows with any nulls\n",
    "df_cleaned = df.na.drop()\n",
    "\n",
    "# Filter out nulls\n",
    "df_cleaned = df.where(col(\"columnName\").isNotNull())\n",
    "\n",
    "# Use .na.fill({\"column\": value) to replace nulls with a specific value\n",
    "# Fill nulls in the age column with the value 0\n",
    "df_filled = df.na.fill({\"age\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4631c",
   "metadata": {},
   "source": [
    "### Column operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use .withColumn() to add a new column based on calculations or existing columns\n",
    "\n",
    "# Create a new column 'age_plus_5'\n",
    "df = df.withColumn(\"age_plus_5\", df[\"age\"] + 5)\n",
    "\n",
    "#Use withColumnRenamed() to rename columns\n",
    "# Rename the 'age' column to 'years'\n",
    "df = df.withColumnRenamed(\"age\", \"years\")\n",
    "\n",
    "#Use drop() to remove unnecessary columns\n",
    "# Drop the 'department' column\n",
    "df = df.drop(\"department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42831c15",
   "metadata": {},
   "source": [
    "### Row operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use .filter() to select rows based on specific conditions\n",
    "# Filter rows where salary is greater than 50000\n",
    "filtered_df = df.filter(df[\"salary\"] > 50000)\n",
    "\n",
    "#Use .groupBy() and aggregate functions (e.g., .sum() , .avg() ) to summarize data\n",
    "# Group by department and calculate the average salary\n",
    "grouped_df = df.groupBy(\"department\").avg(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ef3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any nulls\n",
    "census_cleaned = census_df.na.drop()\n",
    "\n",
    "# Show the result\n",
    "census_cleaned.show()\n",
    "\n",
    "# Create a new column 'weekly_salary'\n",
    "census_df_weekly = census_df.withColumn(\"weekly_salary\", census_df.income / 52)\n",
    "\n",
    "# Rename the 'age' column to 'years'\n",
    "census_df_weekly = census_df_weekly.withColumnRenamed(\"age\", \"years\")\n",
    "\n",
    "# Show the result\n",
    "census_df_weekly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ce968",
   "metadata": {},
   "source": [
    "### Joins in PySpark\n",
    "\n",
    "Syntax\n",
    "\n",
    "```python\n",
    "DataFrame1.join(DataFrame2, on=\"column\", how=\"join_type\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eccb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining on id column using an inner join\n",
    "df_joined = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "\n",
    "# Joining on columns with different names\n",
    "df_joined = df1.join(df2,df1.Id == df2.Name, \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b50448",
   "metadata": {},
   "source": [
    "### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union of two DataFrames with identical schemas\n",
    "df_union = df1.union(df2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
